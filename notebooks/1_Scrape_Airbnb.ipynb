{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "### Getting Airbnb Data\n",
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "#### Download Index File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from os import path\n",
    "import requests\n",
    "\n",
    "airbnb_data_url = \"http://insideairbnb.com/get-the-data.html\"\n",
    "airbnb_data_path = \"../data/airbnb.html\"\n",
    "\n",
    "if not path.exists(airbnb_data_path):\n",
    "    cur_request = requests.get(airbnb_data_url, allow_redirects=True)\n",
    "    open(airbnb_data_path, 'wb').write(cur_request.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start City DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "cur_soup = BeautifulSoup(open(airbnb_data_path), 'html.parser')\n",
    "\n",
    "cur_cities_html = cur_soup.select(\"table.table\")\n",
    "\n",
    "city_dict = {}\n",
    "cur_regex = r\"See (.*) data visually here.\"\n",
    "\n",
    "for cur_city_html in cur_cities_html:\n",
    "    cur_slug = cur_city_html[\"class\"][-1]\n",
    "    \n",
    "    cur_name = re.match(\n",
    "        cur_regex, cur_city_html.findPreviousSibling(\"p\").text\n",
    "    ).groups()[0]\n",
    "    \n",
    "    city_dict[cur_slug] = cur_name\n",
    "\n",
    "city_data = pd.DataFrame(\n",
    "    index=city_dict.keys(), data=city_dict.values(), columns=[\"name\"]\n",
    ")\n",
    "\n",
    "city_data.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Add Rental Urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for cur_anchor in cur_soup.find_all('a', href=True):\n",
    "    cur_anchor.replace_with(cur_anchor['href'])\n",
    "\n",
    "cur_cities_urls = []\n",
    "cur_cities_dates = []\n",
    "cur_cities_geos = []\n",
    "\n",
    "for cur_city_html in cur_cities_html:\n",
    "    cur_city_table = pd.read_html(str(cur_city_html))[0]\n",
    "\n",
    "    cur_city_geo = cur_city_table[cur_city_table[\"File Name\"].str.endswith(\".geojson\")][\"File Name\"].iloc[0]\n",
    "        \n",
    "    cur_cities_geos.append(cur_city_geo)\n",
    "    \n",
    "    cur_city_table = cur_city_table[cur_city_table[\"File Name\"].str.endswith(\"listings.csv.gz\")]\n",
    "\n",
    "    assert cur_city_table[\"Country/City\"].nunique() == 1\n",
    "\n",
    "    cur_city_table[\"Date Compiled\"] = pd.to_datetime(cur_city_table[\"Date Compiled\"], format=\"%d %B, %Y\")\n",
    "\n",
    "    cur_city_table = cur_city_table[cur_city_table[\"Date Compiled\"].dt.year > 2016]\n",
    "\n",
    "    cur_city_table = cur_city_table.drop_duplicates(subset=\"File Name\")\n",
    "    \n",
    "    cur_cities_urls.append(cur_city_table[\"File Name\"].tolist())\n",
    "    cur_cities_dates.append(cur_city_table[\"Date Compiled\"].tolist())\n",
    "    \n",
    "city_data[\"counts\"] = list(map(len,cur_cities_urls))\n",
    "city_data[\"urls\"] = cur_cities_urls\n",
    "city_data[\"dates\"] = cur_cities_dates\n",
    "city_data[\"geo_link\"] = cur_cities_geos\n",
    "\n",
    "top_cities = city_data[city_data.counts >= 12 + 12].sort_values(by=\"counts\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.barh(top_cities.index, top_cities.counts)\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "#### Download Monthly Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def async_run(f, my_iter):\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        results = list(tqdm(\n",
    "            executor.map(f, my_iter), total=len(my_iter)\n",
    "        ))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "from time import sleep\n",
    "\n",
    "def get_month_data(cur_file_tuple):\n",
    "    cur_file_name, cur_file_link = cur_file_tuple\n",
    "    if path.exists(cur_file_name): return\n",
    "    \n",
    "    month_data = pd.read_csv(cur_file_link, low_memory=False)\n",
    "    \n",
    "    month_data = month_data[month_data.room_type == \"Entire home/apt\"]\n",
    "    month_data = month_data[month_data.property_type == \"Apartment\"]\n",
    "    month_data = month_data[month_data.bed_type == \"Real Bed\"]\n",
    "\n",
    "    month_data[\"price\"] = month_data.price.str.replace(\n",
    "        r\"[\\$\\,]\", \"\"\n",
    "    ).astype(float).round(2)\n",
    "\n",
    "    month_data = month_data[[\n",
    "        'id', 'price', 'latitude', 'longitude', \n",
    "        'accommodates', 'bathrooms', 'bedrooms',\n",
    "        'is_location_exact', 'number_of_reviews',\n",
    "        'review_scores_rating', 'reviews_per_month'\n",
    "    ]]\n",
    "\n",
    "    month_data = month_data.reset_index(drop=True)\n",
    "    month_data.to_csv(cur_file_name)\n",
    "    \n",
    "    sleep(0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "used_cities = city_data.loc[[\"san-francisco\", \"new-york-city\", \"paris\", \"berlin\"]]\n",
    "\n",
    "cur_file_names = []\n",
    "cur_file_links = []\n",
    "\n",
    "for cur_row in used_cities.itertuples():\n",
    "    for cur_date, cur_file_link in zip(cur_row.dates, cur_row.urls):\n",
    "        cur_file_name = \"../data/cities/\"\n",
    "        cur_file_name += cur_row.Index.replace('-', '_')\n",
    "        cur_file_name += \"_\"\n",
    "        cur_file_name += cur_date.strftime('%Y_%m_%d')\n",
    "        cur_file_name += \".csv\"\n",
    "        \n",
    "        cur_file_names.append(cur_file_name)\n",
    "        cur_file_links.append(cur_file_link)\n",
    "        \n",
    "        (cur_file_name, cur_file_link)\n",
    "\n",
    "async_run(\n",
    "    get_month_data, \n",
    "    list(zip(cur_file_names, cur_file_links))\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Data Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_work_table(work_table):\n",
    "    work_table = work_table.dropna(subset=[\"price\"])[work_table.price > 0]\n",
    "\n",
    "    id_counts = work_table.groupby(\"id\")[\"id\"].count()\n",
    "    \n",
    "    work_table = pd.merge(\n",
    "        id_counts.to_frame().rename(columns={\"id\": \"weight\"}),\n",
    "        work_table, on=\"id\"\n",
    "    )\n",
    "\n",
    "    bad_ids = id_counts[ id_counts <= 3 ]\n",
    "\n",
    "    work_table = work_table[~work_table.id.isin(bad_ids.index)]\n",
    "\n",
    "    work_table = pd.merge(\n",
    "        work_table.groupby(\"id\")[\"price\"].aggregate(stats.hmean).round(2).to_frame(),\n",
    "        work_table.drop(columns=\"price\").drop_duplicates(subset=\"id\"),\n",
    "        on=\"id\"\n",
    "    ).drop(columns=[\"id\"])\n",
    "\n",
    "    work_table = work_table[work_table.is_location_exact == \"t\"].drop(columns=\"is_location_exact\")\n",
    "\n",
    "    work_table = work_table.dropna()\n",
    "\n",
    "    work_table = work_table[ work_table[\"reviews_per_month\"] > 1/8 ]\n",
    "\n",
    "    work_table = work_table[ work_table[\"number_of_reviews\"] > 5 ]\n",
    "\n",
    "    work_table = work_table[ work_table[\"review_scores_rating\"] > 80 ]\n",
    "\n",
    "    work_table = work_table.drop(columns=[\n",
    "        \"reviews_per_month\",\n",
    "        \"number_of_reviews\", \n",
    "        \"review_scores_rating\"\n",
    "    ])\n",
    "\n",
    "    work_table = work_table.reset_index(drop=True)\n",
    "    \n",
    "    work_table = work_table[work_table.accommodates >= 1]\n",
    "    work_table = work_table[work_table.accommodates <= 8]\n",
    "\n",
    "    work_table = work_table[work_table.bathrooms >= 1 ]\n",
    "    work_table = work_table[work_table.bathrooms <= 4 ]\n",
    "\n",
    "    work_table = work_table[ work_table.bedrooms >= 1 ]\n",
    "    work_table = work_table[ work_table.bedrooms <= 4 ]\n",
    "\n",
    "    price_bin_count = 20\n",
    "\n",
    "    city_price_bins = pd.qcut(work_table.price, price_bin_count, False)\n",
    "\n",
    "    work_table = work_table.loc[\n",
    "        city_price_bins[\n",
    "            ( city_price_bins > 0 ) &\n",
    "            ( city_price_bins < price_bin_count-1 ) \n",
    "        ].index\n",
    "    ]\n",
    "    \n",
    "    work_table[\"bedrooms\"] = work_table.bedrooms.astype(int)\n",
    "\n",
    "    return work_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "rental_data_list = []\n",
    "\n",
    "for cur_row in used_cities.itertuples():\n",
    "    cur_glob = glob.glob(\n",
    "        f\"../data/cities/{cur_row.Index.replace('-', '_')}*.csv\"\n",
    "    )\n",
    "    \n",
    "    assert cur_row.counts == len(cur_glob)\n",
    "    \n",
    "    work_table = None\n",
    "              \n",
    "    for cur_file in cur_glob:\n",
    "        cur_date = datetime.strptime(\n",
    "            re.search(r\"(\\d{4}_\\d{2}_\\d{2})\\.csv\", cur_file).group(1),\n",
    "            '%Y_%m_%d'\n",
    "        )\n",
    "        \n",
    "        work_sub_table = pd.read_csv(cur_file).drop(columns=\"Unnamed: 0\")\n",
    "        work_sub_table[\"date\"] = cur_date\n",
    "\n",
    "        if work_table is None:\n",
    "            work_table = work_sub_table\n",
    "        else:\n",
    "            work_table = pd.concat([work_table, work_sub_table])\n",
    "\n",
    "    work_table = work_table.sort_values(\n",
    "        by=[\"id\", \"date\"], ascending=[True, False]\n",
    "    ).drop(columns=\"date\")\n",
    "    \n",
    "    work_table = clean_work_table(work_table)\n",
    "    \n",
    "    rental_data_list.append(work_table)\n",
    "    \n",
    "used_cities[\"rentals\"] = rental_data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Geo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas \n",
    "from shapely.ops import unary_union\n",
    "\n",
    "geo_data_list = []\n",
    "for cur_row in used_cities.itertuples():\n",
    "    cur_geo_link = city_data.loc[cur_row.Index].geo_link\n",
    "    cur_geo_file = f\"../data/geo/{cur_row.Index.replace('-', '_')}.geojson\"\n",
    "    \n",
    "    if not path.exists(cur_geo_file): \n",
    "        cur_request = requests.get(cur_geo_link, allow_redirects=True)\n",
    "        open(cur_geo_file, 'wb').write(cur_request.content)\n",
    "\n",
    "    tmp_geo_data = geopandas.read_file(cur_geo_file)\n",
    "    \n",
    "    if cur_row.Index == \"new-york-city\":\n",
    "        sub_geo = geopandas.read_file(\n",
    "            \"http://data.ci.newark.nj.us/dataset/db87f66a-6d79-4933-9011-f392fdce7eb8/resource/95db8cad-3a8c-41a4-b8b1-4991990f07f3/download/njcountypolygonv2.geojson\"\n",
    "        )\n",
    "\n",
    "        sub_geo = sub_geo[sub_geo.geometry.centroid.y > 40]\n",
    "        sub_geo = sub_geo[sub_geo.geometry.centroid.x > -75]\n",
    "        \n",
    "        sub_geo = sub_geo[[\"county\", \"statefp\", \"geometry\"]]\n",
    "        sub_geo[\"statefp\"] = \"New Jersey\"\n",
    "        sub_geo.columns = tmp_geo_data.columns\n",
    "        \n",
    "        tmp_geo_data = pd.concat([tmp_geo_data, sub_geo])\n",
    "        \n",
    "    geo_data_list.append(tmp_geo_data)\n",
    "    \n",
    "used_cities[\"geo\"] = geo_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metro_list = []\n",
    "\n",
    "for cur_row in used_cities.itertuples():\n",
    "    cur_metro_file = f\"../data/geo/trains/{cur_row.Index.replace('-', '_')}.geojson\"    \n",
    "    assert path.exists(cur_metro_file)\n",
    "\n",
    "    tmp_metro_data = geopandas.read_file(cur_metro_file)\n",
    "\n",
    "    if cur_row.Index in [\"paris\", \"berlin\"]:\n",
    "        tmp_metro_data = tmp_metro_data[tmp_metro_data.route_type < 3]\n",
    "        \n",
    "    metro_list.append(tmp_metro_data)\n",
    "    \n",
    "used_cities[\"metro\"] = metro_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cities = used_cities.drop(\n",
    "    columns=[\"counts\", \"urls\", \"dates\", \"geo_link\"]\n",
    ")\n",
    "\n",
    "used_cities.to_pickle(\"../data/pickles/cities_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(used_cities.name, list(map(len,used_cities.rentals))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
